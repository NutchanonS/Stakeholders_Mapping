{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iCgEeJ4dNZgm"
      },
      "outputs": [],
      "source": [
        "# https://drive.google.com/file/d/1r13PwwcysckfxC9sEVsFkYMopJEwal0d/view?usp=sharing\n",
        "!gdown --id 1r13PwwcysckfxC9sEVsFkYMopJEwal0d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Te-21gFNli1"
      },
      "outputs": [],
      "source": [
        "!tar -xvf \"/content/loadtestset.tar.gz\" -C \"/content/drive/path/loadtest/\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nB4zsiaHmOP"
      },
      "outputs": [],
      "source": [
        "!pip install pythainlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXiZxT59GP-s"
      },
      "outputs": [],
      "source": [
        "#https://drive.google.com/file/d/1GHW8A0TmN3ZUQf60u7qEGN6kfYOHesx9/view?usp=sharing\n",
        "!pip install -U --no-cache-dir gdown --pre\n",
        "!gdown --id 1GHW8A0TmN3ZUQf60u7qEGN6kfYOHesx9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vuFxKTnGpaG"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1GHW8A0TmN3ZUQf60u7qEGN6kfYOHesx9\n",
        "!unzip \"/content/deploy.zip\" -d \"/content/Model_deploy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD2pjL-O3q0m",
        "outputId": "c9848cca-e93d-4fe3-c77e-66f2175811f9"
      },
      "outputs": [],
      "source": [
        "!pip install requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piFMNZfMAox_"
      },
      "outputs": [],
      "source": [
        "! pip install pythainlp\n",
        "!pip install sklearn_crfsuite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3-Yoq4zHjp-"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import operator\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from pythainlp.corpus import thai_stopwords\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import json\n",
        "import string\n",
        "Stopwords=thai_stopwords()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGNKnG7V2AQh"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5FgArtU25ic"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# json_law = { \"text\" : \n",
        "#   ['วัน', 'เกิด', 'เหตุ', 'จำเลย', 'กับ', 'เพื่อน', ' ', 'ประมาณ', ' ', '5', '-', '6', ' ', 'คน', ' ', 'ได้', 'ไป', 'นั่ง', 'ดื่ม', 'สุรา', 'อยู่', 'ที่', 'บริเวณ', 'สะพาน', 'ใหม่', 'ชลบุรี', ' ', 'และ', 'ระหว่าง', 'นั่ง', 'ดื่ม', 'กัน', 'นั้น', ' ', 'พรรค', 'พวก', 'ผู้', 'ตาย', '(', 'ซึ่ง', 'เป็น', 'พยาน', 'โจทก์', 'ใน', 'คดี', 'นี้', ')', ' ', 'ขี่', 'รถ', 'มอเตอร์ไซค์', 'ผ่าน', 'มา', ' ', 'และ', 'เห็น', 'ว่า', 'จำเลย', ' ', 'ซึ่ง', 'เป็น', 'โจทก์', 'เก่า', 'ของ', 'ตน', 'นั่ง', 'ดื่ม', 'สุรา', 'กัน', 'อยู่', 'พรรค', 'พวก', 'ของ', 'ผู้', 'ตาย', 'จึง', 'ได้', 'ไป', 'ตาม', 'ตัว', 'ผู้', 'ตาย', ' ', 'ซึ่ง', 'เป็น', 'นัก', 'เลง', 'ชื่อ', 'ดัง', 'ใน', 'ย่าน', 'ดัง', 'กล่าว', 'ให้', 'มา', 'เล่นงาน', 'ฝ่าย', 'จำเลย', ' ', 'เพราะ', 'โกรธ', 'แค้น', 'ที่', 'เคย', 'มี', 'เรื่อง', 'กับ', 'พวก', 'จำเลย', 'มา', 'ก่อน', ' ', ' ', 'แต่', 'พยาน', 'โจทก์', 'ก็', 'เกรง', 'กลัว', 'ไม่', 'กล้า', 'เข้า', 'มา', 'ต่อสู้', 'กับ', 'จำเลย', 'ด้วย', 'ตน', 'เอง', ' ', 'จึง', 'ได้', 'ไป', 'ขอ', 'แรง', 'ผู้', 'ตาย', 'ซึ่ง', 'เป็น', 'นัก', 'เลง', 'ใหญ่', 'ให้', 'มา', 'ช่วย', 'จัดการ', ' ', 'ผู้', 'ตาย', 'จึง', 'ได้', 'พา', 'พวก', 'กว่า', ' ', '20', ' ', 'คน', 'ขี่', 'มอเตอร์ไซค์', 'พร้อม', 'ถือ', 'มีด', 'พก', 'คู่ใจ', ' ', 'นำ', 'พรรคพวก', 'พยาน', 'โจทก์', 'เข้า', 'มา', 'ไล่', 'แทง', 'จำเลย', 'กับ', 'พวก', 'ทันที', ' ', 'ปรากฏ', 'ว่า', 'พวก', 'ของ', 'จำเลย', 'คน', 'หนึ่ง', ' ', 'มี', 'อาวุธ', 'ปืน', 'ลูก', 'ซอง', 'ไทย', 'ประดิษฐ์', ' ', '(', 'อีโบ๊ะ', ')', ' ', 'ติด', 'ตัว', 'ไป', 'ด้วย', ' ', ' ', 'จึง', 'ได้', 'ยิง', 'สวน', 'เข้า', 'ไป', 'ถูก', 'ผู้', 'ตาย', ' ', 'ถึง', 'แก่', 'ความ', 'ตาย', 'ลง', 'ตรง', 'ที่', 'เกิด', 'เหตุ', 'นั้น', 'เอง', ' ', 'หลัง', 'จาก', 'นั้น', 'จำเลย', 'ได้', 'หลบหนี', 'ไป', 'พร้อม', 'กับ', 'เพื่อน', 'ซึ่ง', 'เป็น', 'ผู้', 'ยิง', 'ผู้', 'ตาย', ' ', 'และ', 'เพื่อน', 'ของ', 'จำเลย', 'ได้', 'ฝาก', 'ปืน', 'อีโบ๊ะ', 'ดัง', 'กล่าว', 'ไว้', 'กับ', 'จำเลย', ' ', 'เพื่อ', 'เอา', 'ไว้', 'ป้องกัน', 'ตน', 'เอง', 'ทั้งนี้', 'เนื่อง', 'จาก', 'เป็น', 'ที่', 'ทราบ', 'ดี', 'ว่า', ' ', 'ผู้', 'ตาย', 'เป็น', 'นัก', 'เลง', 'ใหญ่', 'มี', 'พรรค', 'พวก', 'เยอะ', ' ', 'และ', 'พวก', 'พยาน', 'โจทก์', 'รู้จัก', 'จำเลย', 'เป็น', 'อย่าง', 'ดี', ' ', 'แต่', 'ไม่', 'รู้จัก', 'เพื่อน', 'ของ', 'จำเลย', 'คน', 'อื่น', 'ๆ', ' ', 'ดัง', 'นั้น', 'น่า', 'จะ', 'มี', 'การ', 'ล้าง', 'แค้น', 'ให้', 'กับ', 'ผู้', 'ตาย', ' ', 'โดย', 'เป้าหมาย', 'ก็', 'ต้อง', 'เป็น', 'ตัว', 'จำเลย', ' ', ' ', 'จำเลย', 'จึง', 'เก็บ', 'อาวุธ', 'ปืน', 'ดัง', 'กล่าว', 'ไว้', 'ป้องกัน', 'ตัว', ' ', ' ', 'ต่อ', 'มา', 'ปรากฎ', 'ว่า', 'ใน', 'คืน', 'เกิด', 'เหตุ', 'นั้น', 'เอง', 'ตำรวจ', 'ก็', 'ได้', 'เข้า', 'ไป', 'ตรวจ', 'ค้น', 'ที่', 'บ้าน', 'ของ', 'จำเลย', ' ', 'พบ', 'อาวุธ', 'ปืน', 'อีโบ๊ะ', 'ของกลาง', 'ที่', 'ใช้', 'เกิด', 'เหตุ', 'ที่', 'บ้าน', 'ของ', 'จำเลย', 'พร้อม', 'จับกุม', 'ตัว', 'จำเลย', ' ', ' ', 'และ', 'ตั้ง', 'ข้อ', 'หา', 'ว่า', 'ร่วม', 'กัน', 'ฆ่า', 'และ', 'ความ', 'ผิด', 'ตาม', ' ', 'พ.ร.บ.', ' ', 'อาวุธ', 'ปืน', 'ฯ', ' ', ' ']\n",
        "#   }\n",
        "\n",
        "# #json -> str\n",
        "# text = json.dumps(json_law)\n",
        "# #str -> dic\n",
        "# text = json.loads(text)\n",
        "# test_samples_X=text['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQvDJKKDOS89",
        "outputId": "45f0f345-967f-4990-cb4b-9c453acf3584"
      },
      "outputs": [],
      "source": [
        "# text=pd.read_csv('/content/ne_test.raw.txt',sep=' /',skip_blank_lines=True)\n",
        "# text.fillna(' ',inplace=True)\n",
        "# text\n",
        "f = open(\"/content/word_test (1).txt\", \"r\")\n",
        "# print(f.read())\n",
        "data_cut = f.read()\n",
        "data_cut = data_cut.replace(\"\\n\",\"\");\n",
        "test_samples_X=data_cut\n",
        "print(data_cut)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZb3G3Bu68R1",
        "outputId": "48fd7408-58fb-4945-c233-e5998b946dcf"
      },
      "outputs": [],
      "source": [
        "from pythainlp import sent_tokenize\n",
        "st_cut = sent_tokenize(data_cut)\n",
        "print(st_cut)\n",
        "data_cut=st_cut"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmr0352g3ow3",
        "outputId": "4586c95d-9bbb-434b-ac5e-f9361b709a58"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "url_st = \"http://nlp1.the-scamper.superai.me:10100/word\"\n",
        "# print(st)\n",
        "sents=[]\n",
        "for i in data_cut:\n",
        "  payload = json.dumps({\n",
        "      \"title\": i\n",
        "  })\n",
        "  headers = {\n",
        "      'Content-Type': 'application/json'\n",
        "  }\n",
        "  res_st_cut = requests.request(\"POST\", url_st, headers=headers, data=payload)\n",
        "  st_cut_json = json.loads(res_st_cut.text)\n",
        "  sents.append(st_cut_json)\n",
        "\n",
        "# nestedlist = st_cut_json[\"sentence\"]\n",
        "\n",
        "\n",
        "# # print(st_cut['sentence'])\n",
        "flatlist=[element for sublist in sents for element in sublist]\n",
        "# print(nestedlist)\n",
        "\n",
        "# st_cut = [''.join(list_st) for list_st in st_cut_json[\"sentence\"]]\n",
        "print(flatlist)\n",
        "test_samples_X=flatlist\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLLkk-kTPMTl",
        "outputId": "42d029d8-a92b-4406-c436-cec4c1c72df3"
      },
      "outputs": [],
      "source": [
        "# text=list(text['\"'])\n",
        "test_samples_X=flatlist\n",
        "print(test_samples_X)\n",
        "sents = list(filter(lambda x: len(x) <= 150, sents))\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VoGUpGk2fK3",
        "outputId": "bd56a66a-85b2-40c3-8ee3-cab0f4ec6765"
      },
      "outputs": [],
      "source": [
        "path_word2index='/content/Model_deploy/word2index.p'\n",
        "path_modelh5='/content/Model_deploy/pop_lst20_UNK.h5'\n",
        "moel_input_dim=150\n",
        "\n",
        "test_samples_X=flatlist\n",
        "sents = list(filter(lambda x: len(x) <= 150, sents))\n",
        "\n",
        "#load word to index (dict)\n",
        "with open(path_word2index, \"rb\") as fh:\n",
        "  word2index = pickle.load(fh)\n",
        "#load model\n",
        "model = tf.keras.models.load_model(path_modelh5)\n",
        "\n",
        "# index --> to tag\n",
        "def logits_to_tokens(sequences, index):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            token_sequence.append(index[np.argmax(categorical)])  \n",
        "        token_sequences.append(token_sequence)\n",
        "    return token_sequences\n",
        "def change_word2index(sentence_list):\n",
        "    test_try=[]\n",
        "    for sent in sentence_list:\n",
        "        s=[]\n",
        "        for word in sent:\n",
        "            try:\n",
        "                a=float(word)+1\n",
        "                s.append(word2index['NUM'])\n",
        "            except:\n",
        "                if word in word2index:\n",
        "                    s.append(word2index[word])\n",
        "                else:\n",
        "                    s.append(word2index[\"UNK\"])\n",
        "        test_try.append(s)\n",
        "    return test_try\n",
        "\n",
        "\n",
        "#create tag to index\n",
        "tags=['B_BRN', 'B_DES', 'B_DTM', 'B_LOC', 'B_MEA', 'B_NUM', 'B_ORG', 'B_PER', \n",
        "      'B_TRM', 'B_TTL', 'E_BRN', 'E_DES', 'E_DTM', 'E_LOC', 'E_MEA', 'E_NUM', \n",
        "      'E_ORG', 'E_PER', 'E_TRM', 'E_TTL', 'I_BRN', 'I_DES', 'I_DTM', 'I_LOC', \n",
        "      'I_MEA', 'I_NUM', 'I_ORG', 'I_PER', 'I_TRM', 'I_TTL', 'O']\n",
        "word2index_save=[]\n",
        "total_word=set()\n",
        "tag2index = {t: i + 1 for i, t in enumerate(tags)}\n",
        "tag2index['PAD'] = 0  \n",
        "\n",
        "# test_samples=[]\n",
        "# for i in range(0,len(test_samples_X)-150,150):\n",
        "#   test_samples.append(test_samples_X[i:i+150])\n",
        "  \n",
        "sentence_list=sents\n",
        "test_try=change_word2index(sentence_list)   #word to index\n",
        "pred=test_try\n",
        "#predict\n",
        "pred = pad_sequences(test_try, moel_input_dim, padding='post')\n",
        "predict = model.predict(pred)\n",
        "predict = logits_to_tokens(predict, {i: t for t, i in tag2index.items()})\n",
        "\n",
        "\n",
        "output_predict=[]\n",
        "for i,sent in enumerate(sentence_list):\n",
        "  for j, word in enumerate(sent):\n",
        "    if word == \"โจทก์\" or word == \"จำเลย\":    \n",
        "            output_predict.append((word , 'B_PER'))\n",
        "    else:\n",
        "        output_predict.append((word , predict[i][j]))\n",
        "\n",
        "\n",
        "print(output_predict)\n",
        "\n",
        "# output_predict=json.dumps(output_predict)\n",
        "# output_predict=json.loads(output_predict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2w31B3G28QW"
      },
      "outputs": [],
      "source": [
        "def find_BPER(output_predict):\n",
        "  bper=set()\n",
        "  for i,w in enumerate(output_predict):\n",
        "    if w[1]=='B_PER': bper.add(w[0])\n",
        "  return bper\n",
        "def find_sent_BPER(sents):\n",
        "  sent_BPER={}\n",
        "  for i,s in enumerate(sents):\n",
        "    for w in s:\n",
        "      if w in bper:\n",
        "        # sent_BPER.append(s)\n",
        "        if w not in sent_BPER:\n",
        "          sent_BPER[w]=[s]\n",
        "        else:\n",
        "          sent_BPER[w].append(s)\n",
        "  return sent_BPER\n",
        "def freq(words):\n",
        "  words = [word.lower() for word in words]\n",
        "  dict_freq = {}\n",
        "  words_unique = []\n",
        "  for word in words:\n",
        "      if word not in words_unique:\n",
        "          words_unique.append(word)\n",
        "  for word in words_unique:\n",
        "      dict_freq[word] = words.count(word)\n",
        "  return dict_freq\n",
        "def tf_score(word,sentence):\n",
        "    freq_sum = 0\n",
        "    word_frequency_in_sentence = 0\n",
        "    len_sentence = len(sentence)\n",
        "    for word_in_sentence in sentence:\n",
        "        if word == word_in_sentence:\n",
        "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
        "    tf =  word_frequency_in_sentence/ len_sentence\n",
        "    return tf\n",
        "def idf_score(no_of_sentences,word,sentences):\n",
        "    no_of_sentence_containing_word = 0\n",
        "    for sentence in sentences:\n",
        "        if word in sentence:\n",
        "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
        "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
        "    return idf\n",
        "def tf_idf_score(tf,idf):\n",
        "    return tf*idf\n",
        "def word_tfidf(dict_freq,word,sentences,sentence):\n",
        "    word_tfidf = []\n",
        "    tf = tf_score(word,sentence)\n",
        "    idf = idf_score(len(sentences),word,sentences)\n",
        "    tf_idf = tf_idf_score(tf,idf)\n",
        "    return tf_idf\n",
        "def sentence_importance(sentence,dict_freq,sentences):\n",
        "     sentence_score = 0\n",
        "     no_of_sentences = len(sentences)\n",
        "     for word in sentence:\n",
        "                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
        "     return sentence_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuyUvS2tIR3H",
        "outputId": "d913bcc5-ee3a-4d98-ed41-ef680cc7e0a5"
      },
      "outputs": [],
      "source": [
        "textt=test_samples_X\n",
        "te=[]\n",
        "for i in textt:\n",
        "  if i!=' ' and i!='' and i not in '!@#$%^&*()_+_':\n",
        "    te.append(i)\n",
        "word_freq = freq(te)\n",
        "# print(tokenized_words,len(tokenized_words),word_freq)\n",
        "\n",
        "# sentence_list = sentence_seg(textt)\n",
        "sentence_list=sents\n",
        "\n",
        "\n",
        "sents=[]\n",
        "for s in sentence_list:\n",
        "  n=[]\n",
        "  for word in s:\n",
        "    if word in word_freq: \n",
        "      n.append(word)\n",
        "  sents.append(n)\n",
        "print(sents)\n",
        "dict_sent={}\n",
        "for i,s in enumerate(sents):\n",
        "  dict_sent[i]=s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl4KlEZ2He-F",
        "outputId": "3d094f61-7f82-45fc-ebb4-296fccf4a59f"
      },
      "outputs": [],
      "source": [
        "def identity_fun(text):\n",
        "    return text\n",
        "tfidf_vectorizer = TfidfVectorizer(analyzer = 'word', #this is default\n",
        "                                   tokenizer=identity_fun, #does no extra tokenizing\n",
        "                                   preprocessor=identity_fun, #no extra preprocessor\n",
        "                                   token_pattern=None)\n",
        "tfidf_vector= tfidf_vectorizer.fit_transform(sents)\n",
        "tfidf_array = np.array(tfidf_vector.todense())\n",
        "df = pd.DataFrame(tfidf_array,columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "def cosine_similarity(df):\n",
        "  corr_word = []\n",
        "  for i in range(len(df.index)):\n",
        "    corr_vector = []\n",
        "    for j in range(len(df.index)):\n",
        "      corr_vector.append(sum(df.iloc[i] * df.iloc[j]))\n",
        "    corr_word.append(corr_vector)\n",
        "  return pd.DataFrame(np.array(corr_word))\n",
        "\n",
        "corr_word=cosine_similarity(df)\n",
        "\n",
        "bper=list(find_BPER(output_predict))\n",
        "sent_BPER=find_sent_BPER(sents)\n",
        "print(sent_BPER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDAn1HcMEkNh",
        "outputId": "8721ca61-cdd2-40b5-b47a-4d68ec3c36d4"
      },
      "outputs": [],
      "source": [
        "def sent2index(bper,sent_BPER,dict_sent):\n",
        "  num_sent_bper = []\n",
        "  map_bper_num={}\n",
        "  # per\n",
        "  for per in bper:\n",
        "    # sen of per\n",
        "    n_sent = [] \n",
        "    for sen in sent_BPER[per]:\n",
        "      # check each dict_sent with sen\n",
        "      for i in dict_sent:\n",
        "        if (dict_sent[i] == sen):\n",
        "          n_sent.append(i)\n",
        "    map_bper_num[per]=list(set(n_sent))\n",
        "    num_sent_bper.append(list(set(n_sent)))\n",
        "  return map_bper_num , num_sent_bper\n",
        "\n",
        "map_bper_num , num_sent_bper = sent2index(bper,sent_BPER,dict_sent)\n",
        "print(map_bper_num)\n",
        "\n",
        "def firstsent2sequence(num_sent_bper,corr_word):\n",
        "  sequences={}\n",
        "  for per in num_sent_bper:\n",
        "    cor_word=corr_word.iloc[per][corr_word.iloc[per] > 0.5].T\n",
        "    for i in per:\n",
        "      s=set()\n",
        "      for j,w in enumerate(cor_word[i]):\n",
        "        if not(pd.isna(w)):\n",
        "          if w!=1 : \n",
        "            s.add(j)\n",
        "      if i not in sequences: sequences[i]=s\n",
        "      else: sequences[i].union(s)\n",
        "  return sequences\n",
        "sequences = firstsent2sequence(num_sent_bper,corr_word)\n",
        "print(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3RvT1TFvSsy"
      },
      "outputs": [],
      "source": [
        "# for i in range(len(map_bper_num['โจทก์'])):\n",
        "#   print(' '.join(dict_sent[map_bper_num['โจทก์'][i]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHXErbEZu933"
      },
      "outputs": [],
      "source": [
        "sent_sequences=set()\n",
        "for f in sequences:\n",
        "  for i in sequences[f]:\n",
        "    sent_sequences.add(i)\n",
        "# for i in map_bper_num:\n",
        "#   for w in map_bper_num[i]:\n",
        "#     sent_sequences.add(w)\n",
        "sents=[]\n",
        "for i in sorted(sent_sequences):\n",
        "  for j in dict_sent[i]:\n",
        "    if 'โจทก์' in dict_sent[i] and 'จำเลย' in dict_sent[i] :\n",
        "      if ''.join(dict_sent[i]) not in sents:\n",
        "        sents.append(''.join(dict_sent[i])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKpXVa6MvnBZ",
        "outputId": "2acd77f0-7785-4e98-c6c2-cbf6d2fca16e"
      },
      "outputs": [],
      "source": [
        "sents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqbVQGM-v2ya",
        "outputId": "2dbde183-f76c-4c48-b848-0d8daaaa6e20"
      },
      "outputs": [],
      "source": [
        "len(sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nCQTTgzT6bY4",
        "outputId": "beef1cc5-02bf-4f2f-ff52-e024907c027e"
      },
      "outputs": [],
      "source": [
        "corr_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVxhFP39-Zom"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "bper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
